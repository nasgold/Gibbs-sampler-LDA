{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg\n",
    "from decimal import *\n",
    "from math import log\n",
    "from prettytable import PrettyTable\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the corpus and go through each doc to get every word instance\n",
    "vidx_corpus = pickle.load(open('reuters-clean-corpus-vidx.p', 'rb'))\n",
    "corpus_vocab = pickle.load(open('reuters-clean-vocab.p', 'rb'))\n",
    "corpus_words = [idx for doc_idxs in vidx_corpus for idx in doc_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holders for dimensions we will reference often\n",
    "n_docs = len(vidx_corpus)\n",
    "n_vocab = len(corpus_vocab)\n",
    "n_words = len(corpus_words)\n",
    "n_docs, n_vocab, n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of topics into which the corpus will separate words and docs \n",
    "n_topics = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirichlet priors (not updated in this implementation, only doing inference)\n",
    "alpha = 1\n",
    "gamma = 1\n",
    "\n",
    "z_mn = [] # maps each word instance in each doc to a topic\n",
    "p_z = np.ones(n_topics) * (1/n_topics) # unit vector from which to randomly sample initial z_mn\n",
    "\n",
    "n_w_k = np.zeros((n_topics, n_vocab)) # count of each vocab word assigned to topic \n",
    "n_k = np.zeros(n_topics) # count of any word assigned to topic\n",
    "n_m_k = np.zeros((n_topics, n_docs)) # count of each doc's words to topic\n",
    "\n",
    "theta = np.zeros((n_docs, n_topics)) # document topic distribution\n",
    "beta = np.zeros((n_vocab, n_topics)) # vocab word topic distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample p_z for each z_mn entry and get the initial counts\n",
    "for m in range(n_docs):\n",
    "    z_m = []\n",
    "    for n in range(len(vidx_corpus[m])):\n",
    "        k = np.random.multinomial(1, p_z).argmax()\n",
    "        z_m.append(k)\n",
    "\n",
    "        vidx = vidx_corpus[m][n]\n",
    "        n_w_k[k, vidx] += 1\n",
    "        n_k[k] += 1\n",
    "        n_m_k[k, m] += 1\n",
    "    \n",
    "    z_mn.append(np.asarray(z_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update p_z for each z_mn and resample, then update the counts\n",
    "def infer_z(n_docs, vidx_corpus, z_mn, n_w_k, n_k, n_m_k):\n",
    "    n_changes = 0\n",
    "    for m in range(n_docs):\n",
    "        for n in range(len(vidx_corpus[m])):\n",
    "            k = z_mn[m][n]\n",
    "\n",
    "            vidx = vidx_corpus[m][n]\n",
    "            n_w_k[k, vidx] -= 1\n",
    "            n_k[k] -= 1\n",
    "            n_m_k[k, m] -= 1\n",
    "\n",
    "            numer = (n_w_k[:, vidx] + gamma) * (n_m_k[:, m] + alpha)\n",
    "            denom = (n_k + n_vocab*gamma) * (len(vidx_corpus[m]) + n_topics*alpha)\n",
    "            p_z = numer / denom\n",
    "            p_z = p_z / p_z.sum()\n",
    "            new_k = np.random.multinomial(1, p_z).argmax()\n",
    "\n",
    "            z_mn[m][n] = new_k\n",
    "            n_w_k[new_k, vidx] += 1\n",
    "            n_k[new_k] += 1\n",
    "            n_m_k[new_k, m] += 1\n",
    "\n",
    "            if new_k != k:\n",
    "                n_changes += 1\n",
    "\n",
    "    return z_mn, n_w_k, n_k, n_m_k, n_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update theta using stats from z\n",
    "def infer_theta(n_docs, n_m_k, alpha, vidx_corpus, n_topics, theta):\n",
    "    for m in range(n_docs):\n",
    "        numer = n_m_k[:, m] + alpha\n",
    "        denom = len(vidx_corpus[m]) + n_topics*alpha\n",
    "        theta[m] = numer / denom\n",
    "            \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update beta using stats from z\n",
    "def infer_beta(n_vocab, n_w_k, n_k, gamma, beta):\n",
    "    for n in range(n_vocab):\n",
    "        numer = n_w_k[:, n] + gamma\n",
    "        denom = n_k + n_vocab*gamma\n",
    "        beta[n] = numer / denom\n",
    "    \n",
    "    return beta      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the log likelihood of the corpus using theta and beta\n",
    "def compute_logp_corpus(n_docs, alpha, vidx_corpus, z_mn, beta, theta):\n",
    "    logp_corpus = Decimal(0)\n",
    "    for m in range(n_docs):\n",
    "        p_doc = Decimal(1)\n",
    "        for n in range(len(vidx_corpus[m])):\n",
    "            vidx = vidx_corpus[m][n]\n",
    "            k = z_mn[m][n]\n",
    "            p_doc *= Decimal(beta[vidx][k]) * Decimal(theta[m][k])\n",
    "        \n",
    "        logp_corpus += p_doc.ln()\n",
    "    \n",
    "    return logp_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top words (by probability) associated with a given topic identified in the corpus\n",
    "def get_topic_words(beta, k, n_top_words, corpus_vocab):\n",
    "    topic_word_probs = beta[:, k]\n",
    "    topic_word_indices = sorted(range(len(topic_word_probs)),\n",
    "                                key=lambda i: topic_word_probs[i],\n",
    "                                reverse=True)[:n_top_words]\n",
    "    topic_words = [(corpus_vocab[i], beta[i, k]) for i in topic_word_indices]\n",
    "        \n",
    "    return topic_words   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functon to pretty print topic words and their probabilities\n",
    "def print_topic_words(n_topics, beta, n_top_words, corpus_vocab, k)\n",
    "    for k in range(n_topics):\n",
    "        top_words = get_topic_words(beta, k, 10, corpus_vocab)\n",
    "        words, probs = zip(*top_words)\n",
    "        formatted_probs = ['{:.4f}'.format(prob) for prob in probs]\n",
    "\n",
    "        print(f'Topic {k} top words:')\n",
    "        pt = PrettyTable()\n",
    "        pt.add_column('Words', words)\n",
    "        pt.add_column('Probabilities', formatted_probs)\n",
    "        print(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple plotting function\n",
    "def plot_logp_corpus(logp_corpus_list):\n",
    "    fig, ax = plt.subplots(constrained_layout=True)\n",
    "    ax.plot(logp_corpus_list)\n",
    "    ax.set_xlim(0, 256)\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel('Corpus Log Likelihood')\n",
    "    ax.set_yticklabels(['{:,}'.format(int(logp)) for logp in ax.get_yticks().tolist()])\n",
    "    plt.savefig('logp_corpus_vs_iters.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform inference and collect the log likelihood at each iteration\n",
    "logp_corpus_list = []\n",
    "for iters in range(2^8):\n",
    "    z_mn, n_w_k, n_k, n_m_k, n_changes = infer_z(n_docs, vidx_corpus, z_mn, n_w_k, n_k, n_m_k)\n",
    "    theta = infer_theta(n_docs, n_m_k, alpha, vidx_corpus, n_topics, theta)\n",
    "    beta = infer_beta(n_vocab, n_w_k, n_k, gamma, beta)\n",
    "    logp_corpus = compute_logp_corpus(n_docs, alpha, vidx_corpus, z_mn, beta, theta)\n",
    "    logp_corpus_list.append(logp_corpus)\n",
    "    \n",
    "    print(f'\\nFinished iteration {iters}...')\n",
    "    print(f'Iteration z_mn topic change count: {n_changes}')\n",
    "    print(f'Iteration logp_corpus: {logp_corpus:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
